version: "3.9"

services:
  minio:
    hostname: minio
    image: "minio/minio"
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - miniodata:/data
    env_file:
      - .env
    networks:
      - data_network

  mc:
    image: minio/mc
    container_name: mc
    hostname: mc
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c " until (/usr/bin/mc alias set minio http://minio:9000/ minio minio123) do echo '...waiting...' && sleep 10; done; /usr/bin/mc mb minio/mlflow; /usr/bin/mc mb minio/lakehouse; tail -f /dev/null;"
    depends_on:
      - minio
    networks:
      - data_network

  spark-master:
    build:
      context: ./infrastructure/spark
      dockerfile: ./Dockerfile
    image: apache/spark:3.3.2
    container_name: "spark-master"
    ports:
      - "7077:7077" # Spark master port
      - "8081:8080" # Spark master web UI port
    expose:
      - "7077"
    environment:
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./config/spark/log4j.properties:/opt/spark/conf/log4j.properties
    command: [ "/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master" ]
    networks:
      - data_network

  spark-worker-1:
    build:
      context: ./infrastructure/spark
      dockerfile: ./Dockerfile
    container_name: "spark-worker-1"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYTHONPATH=/opt/dagster/app/etl_pipeline
    env_file:
      - .env
    depends_on:
      - spark-master
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./config/spark/log4j.properties:/opt/spark/conf/log4j.properties
      - ./src/etl_pipeline:/opt/dagster/app/etl_pipeline
    command: [ "/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077" ]
    networks:
      - data_network

  spark-worker-2:
    build:
      context: ./infrastructure/spark
      dockerfile: ./Dockerfile
    container_name: "spark-worker-2"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYTHONPATH=/opt/dagster/app/etl_pipeline
    env_file:
      - .env
    depends_on:
      - spark-master
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./config/spark/log4j.properties:/opt/spark/conf/log4j.properties
      - ./src/etl_pipeline:/opt/dagster/app/etl_pipeline
    command: [ "/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077" ]
    networks:
      - data_network

  spark-worker-3:
    build:
      context: ./infrastructure/spark
      dockerfile: ./Dockerfile
    container_name: "spark-worker-3"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYTHONPATH=/opt/dagster/app/etl_pipeline
    env_file:
      - .env
    depends_on:
      - spark-master
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./config/spark/log4j.properties:/opt/spark/conf/log4j.properties
      - ./src/etl_pipeline:/opt/dagster/app/etl_pipeline
    command: [ "/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077" ]
    networks:
      - data_network

  spark-notebook:
    build:
      context: ./notebooks
      dockerfile: ./Dockerfile
    container_name: "spark-notebook"
    user: root
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - GRANT_SUDO="yes"
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000/
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
    volumes:
      - ./notebooks/work:/home/jovyan/work
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
    ports:
      - "8888:8888"
      - "4040:4040"
    networks:
      - data_network

  metabase:
    image: metabase/metabase:latest
    container_name: "metabase"
    ports:
      - "3000:3000"
    # env_file:
    #   - .env
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: admin
      MB_DB_PASS: admin
      MB_DB_HOST: de_psql
    healthcheck:
      test: curl --fail -I http://localhost:3000/api/health || exit 1
      interval: 15s
      timeout: 5s
      retries: 5
    # volumes:
    #   - metabasedata:/metabase-data
    networks:
      - data_network

  mysql:
    image: mysql:8.0
    container_name: mysql
    volumes:
      - mysql_h:/var/lib/mysql
      - ./data/raw:/tmp/dataset
      - ./data/db_scripts:/tmp/load_dataset
    ports:
      - "3307:3306"
    # env_file: .env
    environment:
      - MYSQL_DATABASE=mlflowdb
      - MYSQL_ROOT_USER=root
      - MYSQL_USER=admin
      - MYSQL_PASSWORD=admin
      - MYSQL_ROOT_PASSWORD=admin
    command: --local-infile=1
    networks:
      - data_network

  web:
    restart: always
    build: ./infrastructure/mlflow
    image: mlflow
    container_name: mlflow_server
    depends_on:
      - mc
      - mysql
    ports:
      - "7893:5000"
    networks:
      - data_network
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000/
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
    # environment:#   - MLFLOW_S3_ENDPOINT_URL=http://minio:9000/#   - AWS_ACCESS_KEY_ID=minio#   - AWS_SECRET_ACCESS_KEY=minio123
    command: mlflow server --backend-store-uri mysql+pymysql://root:admin@mysql:3306/mlflowdb --default-artifact-root s3://mlflow/ --artifacts-destination s3://mlflow/ --host 0.0.0.0

  mariadb:
    image: mariadb:10.5.16
    container_name: mariadb
    volumes:
      - mariadb:/var/lib/mysql
    ports:
      - "3309:3306"
    env_file:
      - .env
    networks:
      - data_network

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    image: "bitsondatadev/hive-metastore:latest"
    entrypoint: /entrypoint.sh
    ports:
      - "9083:9083"
    volumes:
      - ./config/metastore/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    environment:
      METASTORE_DB_HOSTNAME: mariadb
    networks:
      - data_network
    depends_on:
      - mariadb
      # - minio

  de_psql:
    image: postgres:14-alpine
    container_name: de_psql
    hostname: de_psql
    volumes:
      - postgres_data_h:/var/lib/postgresql/data
      - ./config/postgres/pg_hba.conf:/tmp/pg_hba.conf
      - ./config/postgres/init_metabase.sql:/docker-entrypoint-initdb.d/init_metabase.sql
      # - ./data/db_scripts:/tmp/load_dataset
    command: [ "postgres", "-c", "hba_file=/tmp/pg_hba.conf" ]
    expose:
      - "5432"
    ports:
      - "5432:5432"
    env_file: .env
    networks:
      - data_network

  spark-thrift-server:
    build:
      context: ./infrastructure/spark
      dockerfile: ./Dockerfile
    container_name: "spark-thrift-server"
    restart: always
    depends_on:
      - spark-master
      - hive-metastore
    ports:
      - "4041:4040"
      - "10000:10000"
    command:
      - /bin/sh
      - -c
      - |
        sleep 10 && \
        /opt/spark/bin/spark-submit \
        --master spark://spark-master:7077 \
        --driver-memory 1G \
        --executor-memory 1G \
        --total-executor-cores 1 \
        --driver-java-options '-Dhive.metastore.uris=thrift://hive-metastore:9083' \
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
        --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 \
        spark-internal
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
    networks:
      - data_network

  de_dagster_dagit:
    build:
      context: ./infrastructure/dagster
      dockerfile: Dockerfile
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3001"
      - -w
      - /opt/dagster/dagster_home/workspace.yaml
    container_name: de_dagster_dagit
    ports:
      - "3001:3001"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
      - ./infrastructure/dagster/workspace.yaml:/opt/dagster/dagster_home/workspace.yaml
    env_file:
      - .env
    networks:
      - data_network
    depends_on:
      - de_psql

  de_dagster_daemon:
    build:
      context: ./infrastructure/dagster
      dockerfile: Dockerfile
    entrypoint:
      - dagster-daemon
      - run
    container_name: de_dagster_daemon
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file:
      - .env
    networks:
      - data_network
    depends_on:
      - de_psql

  etl_pipeline:
    build:
      context: ./src/etl_pipeline
      dockerfile: Dockerfile
    container_name: etl_pipeline
    image: etl_pipeline:latest
    user: root
    volumes:
      - ./src/etl_pipeline:/opt/dagster/app/etl_pipeline
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/spark/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./dagster_home:/opt/dagster/dagster_home
      - ./data:/opt/dagster/app/etl_pipeline/data
    env_file:
      - .env
    expose:
      - "4000"
    networks:
      - data_network

  streamlit:
    container_name: streamlit
    build:
      context: ./infrastructure/streamlit
      dockerfile: Dockerfile
    image: streamlit:latest
    ports:
      - "8501:8501"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
    volumes:
      # - './data:/usr/src/app/data:delegated'
      # - './project:/usr/src/app/project:delegated'
      - ./src/dashboard:/app
    # environment:
    #   - USER_ID=1000
    #   - GROUP_ID=1000
    networks:
      - data_network
networks:
  data_network:
    driver: bridge
    name: data_network

# volumes:
#   spark-data:
#     driver: local

volumes:
  postgres_data_h: {}
  mysql_h: {}
  miniodata: {}
  mariadb: {}
  # metabasedata: {}
