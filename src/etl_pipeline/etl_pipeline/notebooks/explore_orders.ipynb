{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "context = None\n",
                "config = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Olist Orders Exploratory Data Analysis\n",
                "This notebook performs exploratory data analysis on the Olist e-commerce orders dataset using PySpark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import *\n",
                "from pyspark.sql.types import *\n",
                "import os\n",
                "import logging\n",
                "\n",
                "# Set py4j logging to ERROR to reduce verbosity\n",
                "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
                "\n",
                "# Initialize Spark Session with Full Configuration\n",
                "# NOTE: This matches the configuration in SparkSessionResource for consistency.\n",
                "spark = (\n",
                "    SparkSession.builder\n",
                "    .appName(\"OrderExploration\")\n",
                "    .master(\"spark://spark-master:7077\")\n",
                "    .config(\"spark.jars\", \n",
                "            \"/opt/spark/jars/delta-core_2.12-2.3.0.jar,\"\n",
                "            \"/opt/spark/jars/hadoop-aws-3.3.2.jar,\"\n",
                "            \"/opt/spark/jars/delta-storage-2.3.0.jar,\"\n",
                "            \"/opt/spark/jars/aws-java-sdk-bundle-1.11.1026.jar,\"\n",
                "            \"/opt/spark/jars/s3-2.18.41.jar,\"\n",
                "            \"/opt/spark/jars/mysql-connector-java-8.0.19.jar\")\n",
                "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
                "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
                "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
                "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n",
                "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n",
                "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
                "    .config(\"spark.hadoop.fs.connection.ssl.enabled\", \"false\")\n",
                "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
                "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
                "    .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse/\")\n",
                "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
                "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
                "    # Distributed Processing Config\n",
                "    .config(\"spark.executor.cores\", \"1\")\n",
                "    .config(\"spark.cores.max\", \"3\")  # Use all 3 workers\n",
                "    .config(\"spark.driver.memory\", \"512m\")\n",
                "    .config(\"spark.executor.memory\", \"512m\")\n",
                "    .config(\"spark.default.parallelism\", \"3\")\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"3\")\n",
                "    # Force consistent Python execution environment\n",
                "    .config(\"spark.pyspark.python\", \"python3\")\n",
                "    .config(\"spark.pyspark.driver.python\", \"python3\")\n",
                "    .enableHiveSupport()\n",
                "    .getOrCreate()\n",
                ")\n",
                "print(\"✓ Spark Session Created Successfully with Full Config\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data from Bronze Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read orders data from bronze layer\n",
                "try:\n",
                "    orders_df = spark.read.table(\"bronze.order\")\n",
                "    # Repartition to distribute work across workers\n",
                "    orders_df = orders_df.repartition(3)\n",
                "    print(f\"✓ Loaded {orders_df.count():,} orders\")\n",
                "except Exception as e:\n",
                "    print(f\"Note: Could not load from bronze.order table. Using sample data instead.\")\n",
                "    print(f\"Error: {e}\")\n",
                "    # Create a small sample for demonstration\n",
                "    orders_df = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if orders_df:\n",
                "    # Show schema\n",
                "    print(\"Schema:\")\n",
                "    orders_df.printSchema()\n",
                "    \n",
                "    # Show first few rows\n",
                "    print(\"\\nSample Data:\")\n",
                "    orders_df.show(5, truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Basic Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if orders_df:\n",
                "    # Count total orders\n",
                "    total_orders = orders_df.count()\n",
                "    print(f\"Total Orders: {total_orders:,}\")\n",
                "    \n",
                "    # Count unique customers\n",
                "    unique_customers = orders_df.select(\"customer_id\").distinct().count()\n",
                "    print(f\"Unique Customers: {unique_customers:,}\")\n",
                "    \n",
                "    # Order status distribution\n",
                "    print(\"\\nOrder Status Distribution:\")\n",
                "    orders_df.groupBy(\"order_status\") \\\n",
                "        .count() \\\n",
                "        .orderBy(col(\"count\").desc()) \\\n",
                "        .show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Temporal Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if orders_df:\n",
                "    # Convert timestamp to date and extract temporal features\n",
                "    orders_enriched = orders_df \\\n",
                "        .withColumn(\"order_date\", to_date(col(\"order_purchase_timestamp\"))) \\\n",
                "        .withColumn(\"order_year\", year(col(\"order_purchase_timestamp\"))) \\\n",
                "        .withColumn(\"order_month\", month(col(\"order_purchase_timestamp\"))) \\\n",
                "        .withColumn(\"order_day_of_week\", dayofweek(col(\"order_purchase_timestamp\")))\n",
                "    \n",
                "    # Orders by year\n",
                "    print(\"Orders by Year:\")\n",
                "    orders_enriched.groupBy(\"order_year\") \\\n",
                "        .count() \\\n",
                "        .orderBy(\"order_year\") \\\n",
                "        .show()\n",
                "    \n",
                "    # Orders by month (latest year)\n",
                "    print(\"\\nOrders by Month (Most Recent Year):\")\n",
                "    latest_year = orders_enriched.agg(max(\"order_year\")).collect()[0][0]\n",
                "    orders_enriched.filter(col(\"order_year\") == latest_year) \\\n",
                "        .groupBy(\"order_month\") \\\n",
                "        .count() \\\n",
                "        .orderBy(\"order_month\") \\\n",
                "        .show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Order Delivery Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if orders_df:\n",
                "    # Calculate delivery time in days\n",
                "    delivery_analysis = orders_df \\\n",
                "        .filter(col(\"order_delivered_customer_date\").isNotNull()) \\\n",
                "        .withColumn(\n",
                "            \"delivery_time_days\",\n",
                "            datediff(col(\"order_delivered_customer_date\"), col(\"order_purchase_timestamp\"))\n",
                "        )\n",
                "    \n",
                "    # Delivery time statistics\n",
                "    print(\"Delivery Time Statistics (in days):\")\n",
                "    delivery_analysis.select(\n",
                "        avg(\"delivery_time_days\").alias(\"avg_delivery_time\"),\n",
                "        min(\"delivery_time_days\").alias(\"min_delivery_time\"),\n",
                "        max(\"delivery_time_days\").alias(\"max_delivery_time\")\n",
                "    ).show()\n",
                "    \n",
                "    # Delivery time distribution\n",
                "    print(\"\\nDelivery Time Distribution:\")\n",
                "    delivery_analysis.groupBy(\n",
                "        when(col(\"delivery_time_days\") <= 7, \"0-7 days\")\n",
                "        .when(col(\"delivery_time_days\") <= 14, \"8-14 days\")\n",
                "        .when(col(\"delivery_time_days\") <= 21, \"15-21 days\")\n",
                "        .otherwise(\"21+ days\")\n",
                "        .alias(\"delivery_range\")\n",
                "    ).count().orderBy(\"count\", ascending=False).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Load and Join with Order Items"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if orders_df:\n",
                "    try:\n",
                "        # Load order items\n",
                "        order_items_df = spark.read.table(\"bronze.orderitem\")\n",
                "        \n",
                "        # Join orders with order items\n",
                "        orders_with_items = orders_df.join(\n",
                "            order_items_df,\n",
                "            on=\"order_id\",\n",
                "            how=\"inner\"\n",
                "        )\n",
                "        \n",
                "        print(\"✓ Successfully joined orders with order items\")\n",
                "        \n",
                "        # Calculate total value per order\n",
                "        print(\"\\nTop 10 Orders by Total Value:\")\n",
                "        orders_with_items.groupBy(\"order_id\") \\\n",
                "            .agg(\n",
                "                sum(\"price\").alias(\"total_price\"),\n",
                "                count(\"*\").alias(\"num_items\")\n",
                "            ) \\\n",
                "            .orderBy(col(\"total_price\").desc()) \\\n",
                "            .show(10)\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not load order items: {str(e).splitlines()[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if orders_df:\n",
                "    # Create summary report\n",
                "    print(\"=\" * 50)\n",
                "    print(\"SUMMARY REPORT\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    summary = orders_df.select(\n",
                "        count(\"*\").alias(\"total_orders\"),\n",
                "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
                "        min(\"order_purchase_timestamp\").alias(\"first_order_date\"),\n",
                "        max(\"order_purchase_timestamp\").alias(\"last_order_date\")\n",
                "    ).collect()[0]\n",
                "    \n",
                "    print(f\"Total Orders: {summary['total_orders']:,}\")\n",
                "    print(f\"Unique Customers: {summary['unique_customers']:,}\")\n",
                "    print(f\"First Order: {summary['first_order_date']}\")\n",
                "    print(f\"Last Order: {summary['last_order_date']}\")\n",
                "    print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark session\n",
                "# spark.stop()\n",
                "print(\"✓ Analysis Complete\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
